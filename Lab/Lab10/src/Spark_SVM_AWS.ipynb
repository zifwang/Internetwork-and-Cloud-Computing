{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_x_train = \"s3://lab10data/X_train.txt\"\n",
    "url_x_test = \"s3://lab10data/X_test.txt\"\n",
    "url_y_train = \"s3://lab10data/y_train.txt\"\n",
    "url_y_test = \"s3://lab10data/y_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return values\n",
    "\n",
    "def parsePoint_2(line):\n",
    "    values = int(line)-1\n",
    "    return values\n",
    "\n",
    "# Get Training Data\n",
    "x_train_str = sc.textFile(url_x_train)\n",
    "train_x = x_train_str.map(parsePoint)\n",
    "train_y_str = sc.textFile(url_y_train)\n",
    "train_y = train_y_str.map(parsePoint_2)\n",
    "# Get Testing Data\n",
    "test_x_str = sc.textFile(url_x_test)\n",
    "test_x = test_x_str.map(parsePoint)\n",
    "test_y_str = sc.textFile(url_y_test)\n",
    "test_y = test_y_str.map(parsePoint_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the training and testing data by labeledPoint\n",
    "def formatData(x,y):\n",
    "    data = []\n",
    "    assert(len(x) == len(y))\n",
    "    for i in range (len(x)):\n",
    "        data.append(LabeledPoint(y[i],x[i]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "parsedData_train = formatData(train_x.collect(),train_y.collect())\n",
    "train_data = sc.parallelize(parsedData_train)\n",
    "parsedData_test = formatData(test_x.collect(),test_y.collect())\n",
    "test_data = sc.parallelize(parsedData_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest.trainClassifier(train_data, numClasses=12, categoricalFeaturesInfo={},\n",
    "                                     numTrees=12, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(model.numTrees())\n",
    "print(model.totalNumNodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions_test = model.predict(test_data.map(lambda x: x.features))\n",
    "predictions_train = model.predict(train_data.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def accuracy(predcitions,y,length):\n",
    "    count = 0\n",
    "    for a, b in zip(predcitions,y):\n",
    "        if a == b:\n",
    "            count += 1\n",
    "    return count/length*100\n",
    "\n",
    "train_accuracy = accuracy(predictions_train.collect(),train_y.collect(),predictions_train.count())\n",
    "test_accuracy = accuracy(predictions_test.collect(),test_y.collect(),predictions_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_accuracy: {}'.format(train_accuracy))\n",
    "print('test_accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAndPredictions = test_data.map(lambda lp: lp.label).zip(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(labelsAndPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
